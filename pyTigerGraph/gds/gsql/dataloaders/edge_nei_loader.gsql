CREATE QUERY edge_nei_loader_{QUERYSUFFIX}(
    INT num_neighbors=10, 
    INT num_hops=2, 
    BOOL shuffle=FALSE,
    STRING filter_by,
    SET<STRING> v_types,
    SET<STRING> e_types,
    SET<STRING> seed_types,
    STRING delimiter,
    INT num_chunks=2,
    INT num_machines=1,
    INT num_segments=20,
    STRING kafka_address="",
    STRING kafka_topic="",
    INT kafka_topic_partitions=1,
    STRING kafka_max_size="104857600",
    INT kafka_timeout=300000,
    STRING security_protocol="",
    STRING sasl_mechanism="",
    STRING sasl_username="",
    STRING sasl_password="",
    STRING ssl_ca_location="",
    STRING ssl_certificate_location="",
    STRING ssl_key_location="",
    STRING ssl_key_password="",
    STRING ssl_endpoint_identification_algorithm="",
    STRING sasl_kerberos_service_name="",
    STRING sasl_kerberos_keytab="",
    STRING sasl_kerberos_principal=""
) SYNTAX V1 { 
    /*
    This query generates the batches of edges and vertices attached to those edges.

    Parameters :
      num_batches    : Number of batches to divide all edges.
      num_neighbors  : Number of neighbors to expand from seeds.
      num_hops       : Number of hops to traverse to get neighbors.
      shuffle        : Whether to shuffle vertices before collecting data.
      filter_by      : A Boolean attribute to determine which edges are included.
      v_types        : Vertex types to be included.
      e_types        : Edge types to be included.
      seed_types     : Edge types to be included as seeds.
      kafka_address  : Address of the Kafka cluster to send data to.
      kafka_topic    : The Kafka topic to send data to.
      kafka_topic_partitions: Number of partitions for the given Kafka topic.
      kafka_max_size : Maximum Kafka message size.
      security_protocol : Security prototol for Kafka.
      sasl_mechanism : Authentication mechanism for Kafka.
      sasl_username  : SASL username for Kafka. 
      sasl_password  : SASL password for Kafka. 
      ssl_ca_location: Path to CA certificate for verifying the Kafka broker key.
    */
    SumAccum<INT> @tmp_id;
    SumAccum<STRING> @@kafka_error;
    SetAccum<VERTEX> @seeds;
    MapAccum<INT, MinAccum<UINT>> @@mid_to_vid; # This tmp accumulator maps machine ID to the smallest vertex ID on the machine.
    MapAccum<INT, UINT> @@mid_to_producer;
    SumAccum<UINT> @kafka_producer_id;

    start = {v_types};
    # Filter seeds if needed
    start = SELECT s
        FROM start:s -(seed_types:e)- v_types:t
        WHERE filter_by is NULL OR e.getAttr(filter_by, "BOOL")
        POST-ACCUM s.@tmp_id = getvid(s)
        POST-ACCUM t.@tmp_id = getvid(t);
    # Shuffle vertex ID if needed
    IF shuffle THEN
        INT num_vertices = start.size();
        res = SELECT s 
            FROM start:s
            POST-ACCUM s.@tmp_id = floor(rand()*num_vertices)
            LIMIT 1;
    END;

    # If using kafka to export 
    IF kafka_address != "" THEN
        # We generate a vertex set that contains exactly one vertex per machine.
        machine_set = 
            SELECT s
            FROM start:s
            ACCUM 
                INT mid = (getvid(s) >> num_segments & 31) % num_machines,
                @@mid_to_vid += (mid -> getvid(s))
            HAVING @@mid_to_vid.get((getvid(s) >> num_segments & 31) % num_machines) == getvid(s);
        @@mid_to_vid.clear();
        # Initialize Kafka producer per machine
        res = SELECT s
            FROM machine_set:s
            ACCUM
                INT mid = (getvid(s) >> num_segments & 31) % num_machines, 
                UINT producer = init_kafka_producer(
                    kafka_address, kafka_max_size, security_protocol, 
                    sasl_mechanism, sasl_username, sasl_password, ssl_ca_location,
                    ssl_certificate_location, ssl_key_location, ssl_key_password,
                    ssl_endpoint_identification_algorithm, sasl_kerberos_service_name,
                    sasl_kerberos_keytab, sasl_kerberos_principal),
                @@mid_to_producer += (mid -> producer);
        res = SELECT s
            FROM start:s
            ACCUM
                INT mid = (getvid(s) >> num_segments & 31) % num_machines,
                s.@kafka_producer_id += @@mid_to_producer.get(mid);
    END;
    
    FOREACH chunk IN RANGE[0, num_chunks-1] DO
        MapAccum<VERTEX, SetAccum<STRING>> @@v_batch;
        MapAccum<VERTEX, SetAccum<STRING>> @@e_batch;
        
        # Collect neighborhood data for each vertex
        seed1 = SELECT s 
            FROM start:s -(seed_types:e)- v_types:t
            WHERE (filter_by IS NULL OR e.getAttr(filter_by, "BOOL")) and ((s.@tmp_id + t.@tmp_id) % num_chunks == chunk)
            ;
        seed2 = SELECT t
            FROM start:s -(seed_types:e)- v_types:t
            WHERE (filter_by IS NULL OR e.getAttr(filter_by, "BOOL")) and ((s.@tmp_id + t.@tmp_id) % num_chunks == chunk)
            ;
        seeds = seed1 UNION seed2;
        seeds = SELECT s
            FROM seeds:s
            POST-ACCUM
                s.@seeds += s,
                {SEEDVERTEXATTRS};
        FOREACH hop IN RANGE[1, num_hops] DO
            seeds = SELECT t
                FROM seeds:s -(e_types:e)- v_types:t 
                SAMPLE num_neighbors EDGE WHEN s.outdegree() >= 1
                ACCUM
                    t.@seeds += s.@seeds,
                    FOREACH tmp_seed in s.@seeds DO 
                        {EDGEATTRS}
                    END;
            seeds = SELECT s
                FROM seeds:s
                POST-ACCUM 
                    FOREACH tmp_seed in s.@seeds DO 
                        {OTHERVERTEXATTRS}
                    END;
        END;
        # Clear all accums
        all_v = {v_types};
        res = SELECT s
            FROM all_v:s
            POST-ACCUM s.@seeds.clear()
            LIMIT 1;

        # Generate output for each edge
        # If use kafka to export
        IF kafka_address != "" THEN 
            res = SELECT s 
                FROM seed1:s -(seed_types:e)- v_types:t
                WHERE (filter_by is NULL OR e.getAttr(filter_by, "BOOL")) and ((s.@tmp_id + t.@tmp_id) % num_chunks == chunk)
                ACCUM
                    INT part_num = (getvid(s)+getvid(t))%kafka_topic_partitions,
                    STRING batch_id = stringify(getvid(s))+"_"+e.type+"_"+stringify(getvid(t)),
                    SET<STRING> tmp_v_batch = @@v_batch.get(s) + @@v_batch.get(t),
                    INT kafka_errcode = write_to_kafka(s.@kafka_producer_id, kafka_topic, part_num, "vertex_batch_"+batch_id, stringify(tmp_v_batch)),
                    IF kafka_errcode!=0 THEN 
                        @@kafka_error += ("Error sending vertex batch for "+batch_id+": "+stringify(kafka_errcode) + "\n")
                    END,
                    SET<STRING> tmp_e_batch = @@e_batch.get(s) + @@e_batch.get(t),
                    {EDGEATTRSKAFKA},
                    kafka_errcode = write_to_kafka(s.@kafka_producer_id, kafka_topic, part_num, "edge_batch_"+batch_id, stringify(tmp_e_batch)),
                    IF kafka_errcode!=0 THEN 
                        @@kafka_error += ("Error sending edge batch for "+batch_id+ ": "+ stringify(kafka_errcode) + "\n")
                    END
                LIMIT 1;
        # Else return as http response
        ELSE
            MapAccum<STRING, STRING> @@v_data;
            MapAccum<STRING, STRING> @@e_data;
            res = SELECT s 
                FROM seed1:s -(seed_types:e)- v_types:t
                WHERE (filter_by is NULL OR e.getAttr(filter_by, "BOOL")) and ((s.@tmp_id + t.@tmp_id) % num_chunks == chunk)
                ACCUM
                    STRING batch_id = stringify(getvid(s))+"_"+e.type+"_"+stringify(getvid(t)),
                    SET<STRING> tmp_v_batch = @@v_batch.get(s) + @@v_batch.get(t),
                    @@v_data += (batch_id -> stringify(tmp_v_batch)),
                    SET<STRING> tmp_e_batch = @@e_batch.get(s) + @@e_batch.get(t),
                    {EDGEATTRSKAFKA},
                    @@e_data += (batch_id -> stringify(tmp_e_batch))
                LIMIT 1;

            FOREACH (k,v) IN @@v_data DO
                PRINT v as vertex_batch, @@e_data.get(k) as edge_batch, k AS seed;
            END;
        END;
    END;
    
    IF kafka_address != "" THEN 
        res = SELECT s
            FROM machine_set:s
            WHERE (getvid(s) >> num_segments & 31) % num_machines == 0
            ACCUM
                FOREACH i IN RANGE[0, kafka_topic_partitions-1] DO
                    INT kafka_errcode = write_to_kafka(s.@kafka_producer_id, kafka_topic, i, "STOP", ""),
                    IF kafka_errcode!=0 THEN 
                        @@kafka_error += ("Error sending STOP signal to topic partition " + stringify(i) + ": " + stringify(kafka_errcode) + "\n")
                    END
                END;

        res = SELECT s
            FROM machine_set:s
            ACCUM
                INT kafka_errcode = close_kafka_producer(s.@kafka_producer_id, kafka_timeout),
                IF kafka_errcode!=0 THEN 
                    @@kafka_error += ("Error shutting down Kafka producer: " + stringify(kafka_errcode) + "\n")
                END;
        PRINT @@kafka_error as kafkaError;
    END;
}