CREATE QUERY nodepiece_loader_{QUERYSUFFIX}(
  SET<VERTEX> input_vertices,
  SET<STRING> v_types,
  SET<STRING> e_types,
  SET<STRING> seed_types,
  BOOL compute_all = TRUE,
  BOOL clear_cache = TRUE,
  BOOL use_cache = TRUE,
  BOOL precompute = FALSE,
  STRING filter_by,
  STRING anchor_attr,
  INT max_distance,
  INT max_anchors,
  INT max_rel_context,
  BOOL shuffle=FALSE,
  STRING delimiter,
  INT num_chunks=2,
  STRING kafka_address="",
  STRING kafka_topic="",
  INT kafka_topic_partitions=1,
  STRING kafka_max_size="104857600",
  INT kafka_timeout=300000,
  STRING security_protocol="",
  STRING sasl_mechanism="",
  STRING sasl_username="",
  STRING sasl_password="",
  STRING ssl_ca_location="",
  STRING ssl_certificate_location="",
  STRING ssl_key_location="",
  STRING ssl_key_password="",
  STRING ssl_endpoint_identification_algorithm="",
  STRING sasl_kerberos_service_name="",
  STRING sasl_kerberos_keytab="",
  STRING sasl_kerberos_principal="",
  INT num_edge_batches=10
) SYNTAX v1{ 
    TYPEDEF TUPLE<INT v_id, INT distance> Distance_Tuple;
    INT num_vertices;
    SumAccum<INT> @tmp_id;
    HeapAccum<Distance_Tuple> (max_anchors, distance ASC) @token_heap;
    SumAccum<STRING> @rel_context_set;
    SumAccum<STRING> @ancs;
    OrAccum @heapFull;

    MapAccum<INT, MinAccum<INT>> @conv_map;
    BOOL cache_empty = FALSE;
    INT distance;

    start = {v_types};
    # Perform fetch operation if desired
    IF clear_cache THEN
        res = SELECT s FROM start:s POST-ACCUM s.{ANCHOR_CACHE_ATTRIBUTE} = s.@conv_map;
    END;
    IF input_vertices.size() != 0 AND NOT compute_all THEN
        seeds = {input_vertices};
        res = SELECT s FROM seeds:s -(e_types)- v_types:t
            ACCUM
              IF s.{ANCHOR_CACHE_ATTRIBUTE}.size() != 0 THEN
                FOREACH (key, val) IN s.{ANCHOR_CACHE_ATTRIBUTE} DO  # s.ANCHOR_CACHE_ATTRIBUTE should be changed to getAttr() when supported
                  s.@token_heap += Distance_Tuple(key, val)
                END
              ELSE
                FOREACH (key, val) IN t.{ANCHOR_CACHE_ATTRIBUTE} DO
                  s.@token_heap += Distance_Tuple(key, val)
                END
              END
            POST-ACCUM
              IF s.@token_heap.size() == 0 THEN
                cache_empty = TRUE
              END;
    ELSE
      cache_empty = TRUE;
    END;

    IF cache_empty THEN  # computing all, shuffle vertices if needed
        ancs = SELECT s 
           FROM start:s 
           WHERE s.getAttr(anchor_attr, "BOOL")
           POST-ACCUM s.@token_heap += Distance_Tuple(getvid(s), 0);
        start = {seed_types};
        IF filter_by IS NOT NULL THEN
            start = SELECT s FROM start:s WHERE s.getAttr(filter_by, "BOOL");
        END;
        IF shuffle THEN
            num_vertices = start.size();
            res = SELECT s 
                FROM start:s
                POST-ACCUM s.@tmp_id = floor(rand()*num_vertices);
        ELSE
            res = SELECT s 
                FROM start:s
                POST-ACCUM s.@tmp_id = getvid(s);
        END;
        FOREACH i IN RANGE [1, max_distance] DO
          LOG(TRUE, "ANCHOR MESSAGE DISTANCE", i);
          FOREACH j IN RANGE [0, num_edge_batches-1] DO
            LOG(TRUE, "ANCHOR BATCH", j);
            ancs = SELECT t
                 FROM ancs:s -(e_types:e)-v_types:t WHERE t.@heapFull == False AND ((s.@tmp_id+t.@tmp_id)*(s.@tmp_id+t.@tmp_id+1)/2+t.@tmp_id)%num_edge_batches == j
                 ACCUM 
                  FOREACH tup IN s.@token_heap DO 
                    t.@token_heap += Distance_Tuple(tup.v_id, i)
                  END
                POST-ACCUM
                  IF s.@token_heap.size() == max_anchors THEN
                    s.@heapFull += TRUE
                  END;
          END;
        END;
    END;

    # Get batch seeds
    IF input_vertices.size()==0 THEN
        start = {seed_types};
        # Filter seeds if needed
        seeds = SELECT s
            FROM start:s
            WHERE filter_by is NULL OR s.getAttr(filter_by, "BOOL");
    ELSE
        start = input_vertices;
        seeds = SELECT s 
                FROM start:s;
    END;

    # Get relational context
    IF max_rel_context > 0 THEN
        seeds = SELECT s FROM seeds:s -(e_types:e)- v_types:t 
            SAMPLE max_rel_context EDGE WHEN s.outdegree() >= max_rel_context
            ACCUM s.@rel_context_set += e.type +" ";
    END;
    
    res = SELECT s 
        FROM seeds:s 
        POST-ACCUM
            FOREACH tup IN s.@token_heap DO
                s.@ancs += stringify(tup.v_id)+":"+stringify(tup.distance)+" ",
                IF use_cache AND cache_empty THEN
                    s.@conv_map += (tup.v_id -> tup.distance)
                END
            END,
            IF (use_cache AND cache_empty) OR precompute THEN
                s.{ANCHOR_CACHE_ATTRIBUTE} = s.@conv_map
            END;

    IF NOT precompute THEN # No Output if precomputing
        # If getting all vertices of given types
        IF input_vertices.size()==0 THEN
            IF kafka_address != "" THEN
                SumAccum<STRING> @@kafka_error;
                # Initialize Kafka producer
                UINT producer = init_kafka_producer(
                    kafka_address, kafka_max_size, security_protocol, 
                    sasl_mechanism, sasl_username, sasl_password, ssl_ca_location,
                    ssl_certificate_location, ssl_key_location, ssl_key_password,
                    ssl_endpoint_identification_algorithm, sasl_kerberos_service_name,
                    sasl_kerberos_keytab, sasl_kerberos_principal);
                
                FOREACH chunk IN RANGE[0, num_chunks-1] DO
                    res = SELECT s FROM seeds:s 
                        WHERE s.@tmp_id % num_chunks == chunk
                        POST-ACCUM
                            {VERTEXATTRSKAFKA}
                        LIMIT 1;
                END;

                FOREACH i IN RANGE[0, kafka_topic_partitions-1] DO
                    INT kafka_errcode = write_to_kafka(producer, kafka_topic, i, "STOP", "");
                    IF kafka_errcode!=0 THEN 
                        @@kafka_error += ("Error sending STOP signal to topic partition " + stringify(i) + ": " + stringify(kafka_errcode) + "\n");
                    END;
                END;

                INT kafka_errcode = close_kafka_producer(producer, kafka_timeout);
                IF kafka_errcode!=0 THEN 
                    @@kafka_error += ("Error shutting down Kafka producer: " + stringify(kafka_errcode) + "\n");
                END;

                PRINT @@kafka_error as kafkaError;
            ELSE # HTTP mode
                FOREACH chunk IN RANGE[0, num_chunks-1] DO
                    ListAccum<STRING> @@v_batch;
                    res = SELECT s 
                        FROM seeds:s
                        WHERE s.@tmp_id % num_chunks == chunk
                        POST-ACCUM
                            {VERTEXATTRSHTTP}
                        LIMIT 1;
                    
                    FOREACH i IN @@v_batch DO
                        PRINT i as data_batch;
                    END;
                END;
            END;    
        ELSE # Else get given vertices
            ListAccum<STRING> @@v_batch;
            MapAccum<UINT, VERTEX> @@id_map;
            MapAccum<UINT, STRING> @@type_map;

            res = SELECT s 
                FROM seeds:s
                POST-ACCUM 
                    {VERTEXATTRSHTTP},
                    @@id_map += (getvid(s) -> s), 
                    @@type_map += (getvid(s) -> s.type);

            FOREACH i IN @@v_batch DO
                PRINT i as data_batch;
            END;
            PRINT @@id_map AS pids, @@type_map AS types;                         
        END;
    END;
}