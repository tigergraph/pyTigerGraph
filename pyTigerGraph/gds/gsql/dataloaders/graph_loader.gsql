CREATE QUERY graph_loader_{QUERYSUFFIX}(
    STRING filter_by,
    SET<STRING> v_types,
    SET<STRING> e_types,
    STRING delimiter,
    BOOL shuffle=FALSE,
    INT num_chunks=2,
    STRING kafka_address="",
    STRING kafka_topic="",
    INT kafka_topic_partitions=1,
    STRING kafka_max_size="104857600",
    INT kafka_timeout=300000,
    STRING security_protocol="",
    STRING sasl_mechanism="",
    STRING sasl_username="",
    STRING sasl_password="",
    STRING ssl_ca_location="",
    STRING ssl_certificate_location="",
    STRING ssl_key_location="",
    STRING ssl_key_password="",
    STRING ssl_endpoint_identification_algorithm="",
    STRING sasl_kerberos_service_name="",
    STRING sasl_kerberos_keytab="",
    STRING sasl_kerberos_principal=""
) SYNTAX V1 { 
    /*
    This query generates the batches of edges and vertices attached to those edges.

    Parameters :
      num_batches    : Number of batches to divide all edges.
      shuffle        : Whether to shuffle vertices before collecting data.
      filter_by      : A Boolean attribute to determine which edges are included.
      v_types        : Vertex types to be included.
      e_types        : Edge types to be included.
      kafka_address  : Address of the Kafka cluster to send data to.
      kafka_topic    : The Kafka topic to send data to.
      security_protocol : Security prototol for Kafka.
      sasl_mechanism : Authentication mechanism for Kafka.
      sasl_username  : SASL username for Kafka. 
      sasl_password  : SASL password for Kafka.
      ssl_ca_location: Path to CA certificate for verifying the Kafka broker key.
    */
    SumAccum<INT> @tmp_id;

    start = {v_types};
    # Filter seeds if needed
    seeds = SELECT s
        FROM start:s -(e_types:e)- v_types:t
        WHERE filter_by is NULL OR e.getAttr(filter_by, "BOOL")
        POST-ACCUM s.@tmp_id = getvid(s)
        POST-ACCUM t.@tmp_id = getvid(t);
    # Shuffle vertex ID if needed
    IF shuffle THEN
        INT num_vertices = seeds.size();
        res = SELECT s 
            FROM seeds:s
            POST-ACCUM s.@tmp_id = floor(rand()*num_vertices)
            LIMIT 1;
    END;

    # Generate batches
    # If using kafka to export
    IF kafka_address != "" THEN
        SumAccum<STRING> @@kafka_error;

        # Initialize Kafka producer
        UINT producer = init_kafka_producer(
            kafka_address, kafka_max_size, security_protocol, 
            sasl_mechanism, sasl_username, sasl_password, ssl_ca_location,
            ssl_certificate_location, ssl_key_location, ssl_key_password,
            ssl_endpoint_identification_algorithm, sasl_kerberos_service_name,
            sasl_kerberos_keytab, sasl_kerberos_principal);
        
        FOREACH chunk IN RANGE[0, num_chunks-1] DO
            res = SELECT s 
                FROM seeds:s -(e_types:e)- v_types:t
                WHERE (filter_by is NULL OR e.getAttr(filter_by, "BOOL")) and ((s.@tmp_id + t.@tmp_id) % num_chunks == chunk)
                ACCUM
                    STRING s_msg = graph_loader_sub_{QUERYSUFFIX}(s, delimiter),
                    STRING t_msg = graph_loader_sub_{QUERYSUFFIX}(t, delimiter),
                    INT kafka_errcode = write_to_kafka(producer, kafka_topic, (getvid(s)+getvid(t))%kafka_topic_partitions, "vertex_batch_" + stringify(getvid(s))+e.type+stringify(getvid(t)), s_msg+t_msg),
                    IF kafka_errcode!=0 THEN 
                        @@kafka_error += ("Error sending vertex data for edge " + stringify(getvid(s))+e.type+stringify(getvid(t)) + ": "+ stringify(kafka_errcode) + "\\n")
                    END,
                    {EDGEATTRSKAFKA}
                LIMIT 1;
        END;

        FOREACH i IN RANGE[0, kafka_topic_partitions-1] DO
            INT kafka_errcode = write_to_kafka(producer, kafka_topic, i, "STOP", "");
            IF kafka_errcode!=0 THEN 
                @@kafka_error += ("Error sending STOP signal to topic partition " + stringify(i) + ": " + stringify(kafka_errcode) + "\n");
            END;
        END;

        INT kafka_errcode = close_kafka_producer(producer, kafka_timeout);
        IF kafka_errcode!=0 THEN 
            @@kafka_error += ("Error shutting down Kafka producer: " + stringify(kafka_errcode) + "\n");
        END;
        PRINT @@kafka_error as kafkaError;
    # Else return as http response 
    ELSE
        FOREACH chunk IN RANGE[0, num_chunks-1] DO
            MapAccum<STRING, STRING> @@v_batch;
            MapAccum<STRING, STRING> @@e_batch;
        
            res = SELECT s 
                FROM seeds:s -(e_types:e)- v_types:t
                WHERE (filter_by is NULL OR e.getAttr(filter_by, "BOOL")) and ((s.@tmp_id + t.@tmp_id) % num_chunks == chunk)
                ACCUM 
                    STRING s_msg = graph_loader_sub_{QUERYSUFFIX}(s, delimiter),
                    STRING t_msg = graph_loader_sub_{QUERYSUFFIX}(t, delimiter),
                    @@v_batch += (stringify(getvid(s))+e.type+stringify(getvid(t)) -> s_msg+t_msg),
                    {EDGEATTRSHTTP}
                LIMIT 1;
        
            FOREACH (k,v) IN @@v_batch DO
                PRINT v as vertex_batch, @@e_batch.get(k) as edge_batch;
            END;
        END;
    END;
}